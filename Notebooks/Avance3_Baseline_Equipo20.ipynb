{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Avance 3 — Baseline (Equipo 20)\n**Proyecto:** Detección / clasificación de anillos en galaxias (imágenes FITS + variables tabulares)  \n**Objetivo del notebook:** Construir un **modelo baseline** reproducible que sirva como **marco de referencia** para iteraciones futuras.\n\n---\n\n## Checklist del feedback (Semana 2) que se atiende aquí\n- ✅ Probar **colores vs escala de grises** y combinaciones de contrastes (muy rojo/muy azul).  \n- ✅ Usar **color index maps** (p. ej. ND-like: (g-r)/(g+r), (r-z)/(r+z), (g-z)/(g+z)).  \n- ✅ Experimentar con **preprocesamientos** (asinh, clipping, reducción de tamaño).  \n- ✅ **Rebalanceo de clases** (class_weight + oversampling opcional).  \n- ✅ Bloque preparado para **integrar un 2º dataset** (si llega).  \n- ✅ Leer archivos **FITS** y analizar.  \n- ✅ Excluir **anillos nucleares y pseudoanillos** (limitación del dataset).\n\n---\n\n## Requerimientos del entregable (Avance 3)\n- 3.1 Definir **medidas de calidad** del modelo.\n- 3.2 Proveer un **baseline** para evaluar viabilidad y mejorar modelos más avanzados.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === (0) Setup ===\n# Si estás en Google Colab, descomenta estas líneas:\n# !pip -q install astropy photutils scikit-image imbalanced-learn\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\n\nfrom astropy.io import fits\nfrom astropy.visualization import make_lupton_rgb\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    accuracy_score,\n    balanced_accuracy_score,\n    f1_score,\n    classification_report,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n)\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\n\n# Opcional (solo si tienes imbalanced-learn):\ntry:\n    from imblearn.over_sampling import RandomOverSampler\n    IMB_AVAILABLE = True\nexcept Exception:\n    IMB_AVAILABLE = False\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Configuración de rutas y archivos\nAjusta `DATA_DIR`, `IMAGES_DIR` y el nombre del catálogo si tu repo usa otras rutas.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === (1) Paths ===\nDATA_DIR = \"./Dataset\"\nIMAGES_DIR = \"./Images\"\nCATALOG_FITS = os.path.join(DATA_DIR, \"dataset.fits\")\n\n# Segundo dataset (placeholder)\nDATASET2_DIR = \"./Dataset2\"\nCATALOG2_FITS = os.path.join(DATASET2_DIR, \"dataset2.fits\")\n\nprint(\"CATALOG_FITS exists:\", os.path.exists(CATALOG_FITS))\nprint(\"IMAGES_DIR exists:\", os.path.exists(IMAGES_DIR))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Carga del catálogo FITS (tabular)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def fits_table_to_df(fits_path: str, ext: int = 1) -> pd.DataFrame:\n    hdul = fits.open(fits_path)\n    try:\n        data = hdul[ext].data\n        df = pd.DataFrame(np.array(data).byteswap().newbyteorder())  # endian-safe\n    finally:\n        hdul.close()\n    return df\n\ndf = fits_table_to_df(CATALOG_FITS, ext=1)\ndf.head(), df.shape\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.1 Mapeo y redefinición de la variable objetivo (4 clases)\nSe descartan **nuclear** y **pseudo** (feedback).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "ANILLOS_MAP = {0:\"Sin\", 2:\"Nuclear\", 4:\"Interno\", 8:\"Externo\", 12:\"Interno+Externo\", 16:\"Pseudo\"}\n\ndef to_4class(anillos_value: int):\n    if anillos_value in (2, 16):\n        return None\n    if anillos_value == 0:\n        return 0\n    if anillos_value == 4:\n        return 1\n    if anillos_value == 8:\n        return 2\n    if anillos_value == 12:\n        return 3\n    return None\n\ndf[\"anillos_label\"] = df[\"anillos\"].map(ANILLOS_MAP).fillna(\"Desconocido\")\ndf[\"ring_class\"] = df[\"anillos\"].apply(to_4class)\n\nprint(\"Distribución original anillos:\\n\", df[\"anillos_label\"].value_counts())\nprint(\"\\nDistribución ring_class (incluye NaN):\\n\", df[\"ring_class\"].value_counts(dropna=False))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.2 Filtro de outliers en `z` (±2σ)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df0 = df.dropna(subset=[\"ring_class\"]).copy()\n\nz_mean = df0[\"z\"].mean()\nz_std = df0[\"z\"].std()\nlower, upper = z_mean - 2*z_std, z_mean + 2*z_std\n\ndf1 = df0[(df0[\"z\"] >= lower) & (df0[\"z\"] <= upper)].copy()\n\nprint(\"Rows after target filter:\", len(df0))\nprint(\"Rows after z ±2σ filter:\", len(df1))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Ingeniería de características tabulares\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df1[\"z_log1p\"] = np.log1p(df1[\"z\"].clip(lower=0))\ndf1[\"z_bin\"] = pd.qcut(df1[\"z\"], q=3, labels=[\"low\",\"mid\",\"high\"])\n\nra_rad = np.deg2rad(df1[\"ra\"])\ndec_rad = np.deg2rad(df1[\"dec\"])\ndf1[\"ra_sin\"] = np.sin(ra_rad)\ndf1[\"ra_cos\"] = np.cos(ra_rad)\ndf1[\"dec_sin\"] = np.sin(dec_rad)\ndf1[\"dec_cos\"] = np.cos(dec_rad)\n\ndf1[[\"objID\",\"ra\",\"dec\",\"z\",\"z_log1p\",\"ra_sin\",\"ra_cos\",\"dec_sin\",\"dec_cos\",\"ring_class\"]].head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Lectura de imágenes FITS y features visuales (baseline)\nProbamos **gray**, **idx** (color index maps) y **mix** (gray + idx), con **reducción de tamaño**.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def safe_div(a, b, eps=1e-6):\n    return a / (b + eps)\n\ndef asinh_norm(x, clip_percentile=99.5):\n    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n    hi = np.percentile(np.abs(x), clip_percentile)\n    if hi <= 0:\n        return np.zeros_like(x, dtype=np.float32)\n    x = np.clip(x, -hi, hi)\n    y = np.arcsinh(x)\n    y = y - y.min()\n    if y.max() > 0:\n        y = y / y.max()\n    return y.astype(np.float32)\n\ndef read_grz_from_fits(fits_path: str):\n    hdul = fits.open(fits_path)\n    try:\n        data = hdul[0].data\n        if data is None:\n            data = hdul[1].data\n        data = np.array(data)\n        if data.ndim == 3:\n            g = data[0].astype(np.float32)\n            r = data[1].astype(np.float32)\n            z = data[2].astype(np.float32)\n            return g, r, z\n        raise ValueError(f\"Formato inesperado: ndim={data.ndim}\")\n    finally:\n        hdul.close()\n\ndef lupton_rgb(g, r, z, Q=10, stretch=0.5):\n    return make_lupton_rgb(r, g, z, Q=Q, stretch=stretch)\n\ndef grayscale_from_rgb(rgb_uint8):\n    rgb = rgb_uint8.astype(np.float32) / 255.0\n    gray = 0.2989*rgb[...,0] + 0.5870*rgb[...,1] + 0.1140*rgb[...,2]\n    return gray.astype(np.float32)\n\ndef resize_nearest(img, out_h=64, out_w=64):\n    in_h, in_w = img.shape[:2]\n    y_idx = (np.linspace(0, in_h-1, out_h)).astype(int)\n    x_idx = (np.linspace(0, in_w-1, out_w)).astype(int)\n    if img.ndim == 2:\n        return img[np.ix_(y_idx, x_idx)]\n    return img[np.ix_(y_idx, x_idx, np.arange(img.shape[2]))]\n\ndef extract_image_features(fits_path: str, size=64, mode=\"mix\"):\n    g, r, z = read_grz_from_fits(fits_path)\n\n    idx_gr = safe_div((g - r), (g + r))\n    idx_rz = safe_div((r - z), (r + z))\n    idx_gz = safe_div((g - z), (g + z))\n\n    g_n = asinh_norm(g); r_n = asinh_norm(r); z_n = asinh_norm(z)\n    idx_gr_n = asinh_norm(idx_gr); idx_rz_n = asinh_norm(idx_rz); idx_gz_n = asinh_norm(idx_gz)\n\n    rgb = lupton_rgb(g_n, r_n, z_n)  # uint8\n    rgb_rs = resize_nearest(rgb, size, size)\n\n    gray = grayscale_from_rgb(rgb_rs)\n    gray_rs = resize_nearest(gray, size, size)\n\n    idx_stack = np.stack([idx_gr_n, idx_rz_n, idx_gz_n], axis=-1)\n    idx_rs = resize_nearest(idx_stack, size, size)\n\n    if mode == \"rgb\":\n        return (rgb_rs.astype(np.float32) / 255.0).reshape(-1)\n    if mode == \"gray\":\n        return gray_rs.reshape(-1)\n    if mode == \"idx\":\n        return idx_rs.reshape(-1)\n    if mode == \"mix\":\n        return np.concatenate([gray_rs.reshape(-1), idx_rs.reshape(-1)], axis=0)\n\n    raise ValueError(\"mode must be one of: rgb, gray, idx, mix\")\n\nprint(\"Image feature functions ready.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 4.2 Unir catálogo con imágenes locales\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df1[\"objID_str\"] = df1[\"objID\"].astype(str).str.replace(\".0\", \"\", regex=False)\n\ndef find_img_path(objid_str):\n    candidate = os.path.join(IMAGES_DIR, f\"{objid_str}.fits\")\n    return candidate if os.path.exists(candidate) else None\n\ndf1[\"img_path\"] = df1[\"objID_str\"].apply(find_img_path)\nprint(\"Con imagen:\", df1[\"img_path\"].notna().sum(), \"/\", len(df1))\n\ndf_img = df1.dropna(subset=[\"img_path\"]).copy()\ndf_img[[\"objID_str\",\"img_path\",\"ring_class\"]].head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Métricas y modelos baseline\n**Métricas principales (clases desbalanceadas):** Balanced Accuracy y F1-macro.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "TARGET = \"ring_class\"\nTAB_FEATURES = [\"z\", \"z_log1p\", \"ra_sin\", \"ra_cos\", \"dec_sin\", \"dec_cos\"]\n\ndf_mod = df_img.dropna(subset=TAB_FEATURES + [TARGET]).copy()\nX_tab = df_mod[TAB_FEATURES].astype(float)\ny = df_mod[TARGET].astype(int)\n\nX_train_tab, X_test_tab, y_train, y_test, df_train, df_test = train_test_split(\n    X_tab, y, df_mod, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n)\n\nprint(\"Train class dist:\", Counter(y_train))\nprint(\"Test  class dist:\", Counter(y_test))\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def eval_model(name, model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    pred_tr = model.predict(X_train)\n    pred_te = model.predict(X_test)\n    return {\n        \"model\": name,\n        \"acc_train\": accuracy_score(y_train, pred_tr),\n        \"acc_test\": accuracy_score(y_test, pred_te),\n        \"bal_acc_train\": balanced_accuracy_score(y_train, pred_tr),\n        \"bal_acc_test\": balanced_accuracy_score(y_test, pred_te),\n        \"f1m_train\": f1_score(y_train, pred_tr, average=\"macro\"),\n        \"f1m_test\": f1_score(y_test, pred_te, average=\"macro\"),\n    }, pred_te\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Baseline A: Tabular-only\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "models_tab = []\n\ndummy_mf = DummyClassifier(strategy=\"most_frequent\", random_state=RANDOM_STATE)\ndummy_strat = DummyClassifier(strategy=\"stratified\", random_state=RANDOM_STATE)\n\nlogreg_tab = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"clf\", LogisticRegression(\n        max_iter=2000,\n        class_weight=\"balanced\",\n        random_state=RANDOM_STATE\n    ))\n])\n\nrf_tab = RandomForestClassifier(\n    n_estimators=400,\n    class_weight=\"balanced_subsample\",\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\n\nfor name, m in [\n    (\"Dummy-most_frequent (tab)\", dummy_mf),\n    (\"Dummy-stratified (tab)\", dummy_strat),\n    (\"LogReg-balanced (tab)\", logreg_tab),\n    (\"RF-balanced (tab)\", rf_tab),\n]:\n    res, _ = eval_model(name, m, X_train_tab, y_train, X_test_tab, y_test)\n    models_tab.append(res)\n\npd.DataFrame(models_tab).sort_values(\"bal_acc_test\", ascending=False)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Baseline B: Imagen + (opcional) Tabular\nEn esta versión usamos solo la imagen (pixeles) para simplificar.  \nEn iteración futura pueden concatenar `X_tab` a la matriz de imagen.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def build_image_matrix(df_subset: pd.DataFrame, size=64, mode=\"mix\"):\n    feats = []\n    ok = []\n    for i, p in enumerate(df_subset[\"img_path\"].tolist()):\n        try:\n            feats.append(extract_image_features(p, size=size, mode=mode))\n            ok.append(i)\n        except Exception:\n            continue\n    X = np.vstack(feats) if len(feats) else np.empty((0,0))\n    return X, df_subset.iloc[ok].copy()\n\nSIZE = 64\n\nXtr_gray, dftr_gray = build_image_matrix(df_train, size=SIZE, mode=\"gray\")\nXte_gray, dfte_gray = build_image_matrix(df_test,  size=SIZE, mode=\"gray\")\n\nXtr_idx,  dftr_idx  = build_image_matrix(df_train, size=SIZE, mode=\"idx\")\nXte_idx,  dfte_idx  = build_image_matrix(df_test,  size=SIZE, mode=\"idx\")\n\nXtr_mix,  dftr_mix  = build_image_matrix(df_train, size=SIZE, mode=\"mix\")\nXte_mix,  dfte_mix  = build_image_matrix(df_test,  size=SIZE, mode=\"mix\")\n\nytr_gray = dftr_gray[TARGET].astype(int).to_numpy()\nyte_gray = dfte_gray[TARGET].astype(int).to_numpy()\n\nytr_idx  = dftr_idx[TARGET].astype(int).to_numpy()\nyte_idx  = dfte_idx[TARGET].astype(int).to_numpy()\n\nytr_mix  = dftr_mix[TARGET].astype(int).to_numpy()\nyte_mix  = dfte_mix[TARGET].astype(int).to_numpy()\n\nprint(\"mix shapes:\", Xtr_mix.shape, Xte_mix.shape)\nprint(\"Train class dist (mix):\", Counter(ytr_mix))\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Rebalanceo: class_weight + oversampling opcional\ndef maybe_oversample(X, y):\n    if not IMB_AVAILABLE:\n        return X, y, False\n    ros = RandomOverSampler(random_state=RANDOM_STATE)\n    X2, y2 = ros.fit_resample(X, y)\n    return X2, y2, True\n\nlogreg_img = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"clf\", LogisticRegression(\n        max_iter=2000,\n        class_weight=\"balanced\",\n        random_state=RANDOM_STATE\n    ))\n])\n\nrf_img = RandomForestClassifier(\n    n_estimators=400,\n    class_weight=\"balanced_subsample\",\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def run_suite(Xtr, ytr, Xte, yte, label):\n    rows = []\n    preds = {}\n\n    dummy = DummyClassifier(strategy=\"stratified\", random_state=RANDOM_STATE)\n    res, p = eval_model(f\"Dummy-stratified ({label})\", dummy, Xtr, ytr, Xte, yte)\n    rows.append(res); preds[res[\"model\"]] = p\n\n    res, p = eval_model(f\"LogReg-balanced ({label})\", logreg_img, Xtr, ytr, Xte, yte)\n    rows.append(res); preds[res[\"model\"]] = p\n\n    res, p = eval_model(f\"RF-balanced ({label})\", rf_img, Xtr, ytr, Xte, yte)\n    rows.append(res); preds[res[\"model\"]] = p\n\n    return pd.DataFrame(rows).sort_values(\"bal_acc_test\", ascending=False), preds\n\nres_gray, _ = run_suite(Xtr_gray, ytr_gray, Xte_gray, yte_gray, f\"gray@{SIZE}\")\nres_idx,  _ = run_suite(Xtr_idx,  ytr_idx,  Xte_idx,  yte_idx,  f\"idx@{SIZE}\")\nres_mix,  _ = run_suite(Xtr_mix,  ytr_mix,  Xte_mix,  yte_mix,  f\"mix@{SIZE}\")\n\nres_gray, res_idx, res_mix\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Reporte visual del baseline final (mix)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def show_report(model, Xtr, ytr, Xte, yte, title):\n    model.fit(Xtr, ytr)\n    pred_te = model.predict(Xte)\n\n    print(title)\n    print(\"\\nBalanced Acc:\", balanced_accuracy_score(yte, pred_te))\n    print(\"F1-macro    :\", f1_score(yte, pred_te, average=\"macro\"))\n    print(\"\\nClassification report:\\n\", classification_report(yte, pred_te, digits=3))\n\n    cm = confusion_matrix(yte, pred_te)\n    disp = ConfusionMatrixDisplay(cm)\n    disp.plot(values_format=\"d\")\n    plt.title(title)\n    plt.show()\n\n# Oversampling solo si está disponible\nif IMB_AVAILABLE:\n    Xtr_use, ytr_use, used = (*maybe_oversample(Xtr_mix, ytr_mix),)\n    Xtr_use, ytr_use, used = Xtr_use[0], Xtr_use[1], Xtr_use[2]\nelse:\n    Xtr_use, ytr_use, used = Xtr_mix, ytr_mix, False\n\ntitle = f\"Baseline final: LogReg mix@{SIZE} ({'ROS' if used else 'class_weight'})\"\nshow_report(logreg_img, Xtr_use, ytr_use, Xte_mix, yte_mix, title)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) Importancia de características (Permutation Importance)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "logreg_img.fit(Xtr_use, ytr_use)\n\nsample_n = min(300, Xte_mix.shape[0])\nX_imp = Xte_mix[:sample_n]\ny_imp = yte_mix[:sample_n]\n\nperm = permutation_importance(\n    logreg_img, X_imp, y_imp,\n    scoring=\"balanced_accuracy\",\n    n_repeats=10,\n    random_state=RANDOM_STATE\n)\n\nimportances = perm.importances_mean\ntop_idx = np.argsort(importances)[::-1][:20]\n\npd.DataFrame({\n    \"feature_index\": top_idx,\n    \"importance_mean\": importances[top_idx]\n})\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10) Evidencia de sub/sobreajuste: Validación cruzada\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def cv_scores(model, X, y, label):\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n    scoring = {\"bal_acc\":\"balanced_accuracy\", \"f1_macro\":\"f1_macro\", \"acc\":\"accuracy\"}\n    scores = cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=True)\n    out = {\n        \"label\": label,\n        \"train_bal_acc_mean\": np.mean(scores[\"train_bal_acc\"]),\n        \"val_bal_acc_mean\": np.mean(scores[\"test_bal_acc\"]),\n        \"train_f1m_mean\": np.mean(scores[\"train_f1_macro\"]),\n        \"val_f1m_mean\": np.mean(scores[\"test_f1_macro\"]),\n    }\n    return out\n\ncv_out = cv_scores(logreg_img, Xtr_mix, ytr_mix, f\"LogReg mix@{SIZE} (class_weight)\")\npd.DataFrame([cv_out])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11) Desempeño mínimo\nComparación contra Dummy-stratified.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "dummy = DummyClassifier(strategy=\"stratified\", random_state=RANDOM_STATE)\ndummy.fit(Xtr_mix, ytr_mix)\npred_dummy = dummy.predict(Xte_mix)\n\nmin_bal = balanced_accuracy_score(yte_mix, pred_dummy)\nmin_f1m = f1_score(yte_mix, pred_dummy, average=\"macro\")\n\nlogreg_img.fit(Xtr_use, ytr_use)\npred_lr = logreg_img.predict(Xte_mix)\n\nbal_lr = balanced_accuracy_score(yte_mix, pred_lr)\nf1m_lr = f1_score(yte_mix, pred_lr, average=\"macro\")\n\nprint(\"Dummy Balanced Acc:\", min_bal, \" Dummy F1-macro:\", min_f1m)\nprint(\"LR    Balanced Acc:\", bal_lr, \" LR    F1-macro:\", f1m_lr)\nprint(\"Δ Balanced Acc:\", bal_lr - min_bal)\nprint(\"Δ F1-macro    :\", f1m_lr - min_f1m)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 12) Bloque preparado para integrar un 2º dataset\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def load_and_prepare_catalog(fits_path: str) -> pd.DataFrame:\n    df_ = fits_table_to_df(fits_path, ext=1)\n    df_[\"ring_class\"] = df_[\"anillos\"].apply(to_4class)\n    df_ = df_.dropna(subset=[\"ring_class\"]).copy()\n\n    z_mean_ = df_[\"z\"].mean()\n    z_std_ = df_[\"z\"].std()\n    lower_, upper_ = z_mean_ - 2*z_std_, z_mean_ + 2*z_std_\n    df_ = df_[(df_[\"z\"] >= lower_) & (df_[\"z\"] <= upper_)].copy()\n\n    df_[\"z_log1p\"] = np.log1p(df_[\"z\"].clip(lower=0))\n    ra_rad_ = np.deg2rad(df_[\"ra\"])\n    dec_rad_ = np.deg2rad(df_[\"dec\"])\n    df_[\"ra_sin\"] = np.sin(ra_rad_)\n    df_[\"ra_cos\"] = np.cos(ra_rad_)\n    df_[\"dec_sin\"] = np.sin(dec_rad_)\n    df_[\"dec_cos\"] = np.cos(dec_rad_)\n\n    df_[\"objID_str\"] = df_[\"objID\"].astype(str).str.replace(\".0\", \"\", regex=False)\n    return df_\n\nif os.path.exists(CATALOG2_FITS):\n    df2 = load_and_prepare_catalog(CATALOG2_FITS)\n    print(\"Dataset2 loaded:\", df2.shape)\n    print(\"Class dist ds2:\", Counter(df2[\"ring_class\"].astype(int)))\nelse:\n    print(\"Dataset2 no encontrado aún. (OK)\")\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}